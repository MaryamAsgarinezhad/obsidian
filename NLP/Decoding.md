We are going to explain the steps after encoding and applying attention:

- In the decoders, we use a **fully conneted** layer. It's input is the token vector representation arguments, and it has one ouput for each token in the vocabulary(each possible word)

![[Pasted image 20240525151420.png]]

------------------------------------------------

GPT vs Transformer:

![[Pasted image 20240525153646.png]]