We are going to explain the steps after encoding and applying attention:

- In the decoders, we use a fully conneted layer. It's input is the token vector representation arguments, and it has one ouput for each token in the vocabulary.

![[Pasted image 20240525150833.png]]